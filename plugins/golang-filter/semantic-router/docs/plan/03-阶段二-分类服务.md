# 阶段二: 分类服务开发

## 目标

开发独立的 gRPC 分类服务，基于 ModernBERT 模型实现语义分类功能。

**预期成果**:
- ✅ gRPC 分类服务可独立运行
- ✅ 支持接收文本并返回类别 + 置信度
- ✅ 模型推理性能满足要求 (< 30ms p99)
- ✅ Higress 插件可通过 gRPC 调用分类服务

**预计时间**: 4-5 天

---

## 任务清单

### Task 2.1: gRPC 接口定义

**目标**: 定义分类服务的 gRPC 接口协议

**Protocol Buffer 定义** (`proto/classifier.proto`):
```protobuf
syntax = "proto3";

package semantic_router;

option go_package = "github.com/alibaba/higress/plugins/golang-filter/semantic-router/proto";

// 分类服务
service ClassifierService {
  // 单条文本分类
  rpc Classify(ClassifyRequest) returns (ClassifyResponse);

  // 批量文本分类 (可选优化)
  rpc ClassifyBatch(ClassifyBatchRequest) returns (ClassifyBatchResponse);

  // 健康检查
  rpc HealthCheck(HealthCheckRequest) returns (HealthCheckResponse);
}

// 分类请求
message ClassifyRequest {
  string text = 1;                    // 用户问题文本
  bool enable_cache = 2;              // 是否启用缓存
  map<string, string> metadata = 3;   // 附加元数据
}

// 分类响应
message ClassifyResponse {
  string category = 1;                     // 分类类别
  double confidence = 2;                   // 置信度 (0-1)
  map<string, double> scores = 3;          // 所有类别的得分
  string method = 4;                       // 分类方法 (model/cache/fallback)
  int64 latency_ms = 5;                    // 推理延迟(毫秒)
}

// 批量分类请求
message ClassifyBatchRequest {
  repeated string texts = 1;
  bool enable_cache = 2;
}

// 批量分类响应
message ClassifyBatchResponse {
  repeated ClassifyResponse responses = 1;
}

// 健康检查
message HealthCheckRequest {}

message HealthCheckResponse {
  bool healthy = 1;
  string model_status = 2;
  int64 total_requests = 3;
  double avg_latency_ms = 4;
}
```

**生成代码**:
```bash
# 安装 protoc 编译器
# macOS: brew install protobuf
# Linux: apt-get install protobuf-compiler

# 安装 Go 插件
go install google.golang.org/protobuf/cmd/protoc-gen-go@latest
go install google.golang.org/grpc/cmd/protoc-gen-go-grpc@latest

# 生成代码
cd plugins/golang-filter/semantic-router
protoc --go_out=. --go-grpc_out=. proto/classifier.proto
```

**验收标准**:
- ✅ Protocol Buffer 定义完整
- ✅ Go 代码生成成功
- ✅ 接口设计合理

---

### Task 2.2: Python 模型推理服务

**目标**: 实现基于 ModernBERT 的分类模型推理

**项目结构**:
```
classifier-service/
├── main.py                    # 主入口
├── server.py                  # gRPC 服务器
├── classifier.py              # 模型推理逻辑
├── config.py                  # 配置管理
├── requirements.txt
├── Dockerfile
└── models/                    # 模型文件目录
    └── category_classifier/
        ├── config.json
        ├── pytorch_model.bin
        └── vocab.txt
```

**依赖** (`requirements.txt`):
```txt
grpcio==1.60.0
grpcio-tools==1.60.0
transformers==4.36.0
torch==2.1.0
numpy==1.24.3
protobuf==4.25.1
peft==0.7.0                    # LoRA 支持
```

**配置** (`config.py`):
```python
from dataclasses import dataclass
from typing import Dict

@dataclass
class ModelConfig:
    model_path: str = "./models/category_classifier"
    device: str = "cpu"                    # 或 "cuda"
    max_length: int = 512
    batch_size: int = 8
    num_labels: int = 10

@dataclass
class ServerConfig:
    host: str = "0.0.0.0"
    port: int = 50051
    max_workers: int = 10
    enable_cache: bool = True
    cache_size: int = 1000

@dataclass
class CategoryConfig:
    categories: list = None

    def __post_init__(self):
        if self.categories is None:
            self.categories = [
                "math",
                "code",
                "medicine",
                "writing",
                "physics",
                "chemistry",
                "biology",
                "engineering",
                "law",
                "general"
            ]
```

**分类器** (`classifier.py`):
```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from typing import Dict, Tuple
import time
import logging

logger = logging.getLogger(__name__)

class SemanticClassifier:
    def __init__(self, config):
        self.config = config
        self.device = torch.device(config.device)

        # 加载分词器
        logger.info(f"Loading tokenizer from {config.model_path}")
        self.tokenizer = AutoTokenizer.from_pretrained(config.model_path)

        # 加载模型
        logger.info(f"Loading model from {config.model_path}")
        self.model = AutoModelForSequenceClassification.from_pretrained(
            config.model_path,
            num_labels=config.num_labels
        )
        self.model.to(self.device)
        self.model.eval()

        # 类别映射
        self.id2label = self.model.config.id2label
        self.label2id = self.model.config.label2id

        logger.info(f"Model loaded successfully on {self.device}")

    def classify(self, text: str) -> Tuple[str, float, Dict[str, float]]:
        """
        分类单条文本

        返回:
            category: 预测类别
            confidence: 置信度
            scores: 所有类别的得分
        """
        start_time = time.time()

        # 分词
        inputs = self.tokenizer(
            text,
            max_length=self.config.max_length,
            padding=True,
            truncation=True,
            return_tensors="pt"
        ).to(self.device)

        # 推理
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=-1)

        # 获取预测结果
        probs = probabilities[0].cpu().numpy()
        pred_idx = probs.argmax()
        category = self.id2label[pred_idx]
        confidence = float(probs[pred_idx])

        # 所有类别得分
        scores = {
            self.id2label[i]: float(probs[i])
            for i in range(len(probs))
        }

        latency = (time.time() - start_time) * 1000
        logger.debug(f"Classification: {category} ({confidence:.3f}) in {latency:.1f}ms")

        return category, confidence, scores

    def classify_batch(self, texts: list) -> list:
        """批量分类"""
        start_time = time.time()

        # 分词
        inputs = self.tokenizer(
            texts,
            max_length=self.config.max_length,
            padding=True,
            truncation=True,
            return_tensors="pt"
        ).to(self.device)

        # 推理
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=-1)

        # 解析结果
        results = []
        probs = probabilities.cpu().numpy()

        for prob in probs:
            pred_idx = prob.argmax()
            category = self.id2label[pred_idx]
            confidence = float(prob[pred_idx])
            scores = {
                self.id2label[i]: float(prob[i])
                for i in range(len(prob))
            }
            results.append((category, confidence, scores))

        latency = (time.time() - start_time) * 1000
        logger.debug(f"Batch classification: {len(texts)} texts in {latency:.1f}ms")

        return results
```

**gRPC 服务器** (`server.py`):
```python
import grpc
from concurrent import futures
import logging
import time
from functools import lru_cache

# 导入生成的 protobuf 代码
import classifier_pb2
import classifier_pb2_grpc

from classifier import SemanticClassifier
from config import ModelConfig, ServerConfig

logger = logging.getLogger(__name__)

class ClassifierServicer(classifier_pb2_grpc.ClassifierServiceServicer):
    def __init__(self, model_config, server_config):
        self.classifier = SemanticClassifier(model_config)
        self.server_config = server_config

        # 统计信息
        self.total_requests = 0
        self.total_latency = 0

        # 缓存装饰器
        if server_config.enable_cache:
            self._cached_classify = lru_cache(maxsize=server_config.cache_size)(
                self._classify_impl
            )
        else:
            self._cached_classify = self._classify_impl

    def _classify_impl(self, text: str):
        """实际的分类实现"""
        return self.classifier.classify(text)

    def Classify(self, request, context):
        """单条分类"""
        start_time = time.time()
        self.total_requests += 1

        try:
            # 调用分类器 (带缓存)
            if request.enable_cache and self.server_config.enable_cache:
                category, confidence, scores = self._cached_classify(request.text)
                method = "cache"
            else:
                category, confidence, scores = self.classifier.classify(request.text)
                method = "model"

            latency_ms = int((time.time() - start_time) * 1000)
            self.total_latency += latency_ms

            return classifier_pb2.ClassifyResponse(
                category=category,
                confidence=confidence,
                scores=scores,
                method=method,
                latency_ms=latency_ms
            )

        except Exception as e:
            logger.error(f"Classification failed: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return classifier_pb2.ClassifyResponse()

    def ClassifyBatch(self, request, context):
        """批量分类"""
        try:
            results = self.classifier.classify_batch(request.texts)

            responses = []
            for category, confidence, scores in results:
                responses.append(classifier_pb2.ClassifyResponse(
                    category=category,
                    confidence=confidence,
                    scores=scores,
                    method="model",
                    latency_ms=0  # 批量模式不单独计时
                ))

            return classifier_pb2.ClassifyBatchResponse(responses=responses)

        except Exception as e:
            logger.error(f"Batch classification failed: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return classifier_pb2.ClassifyBatchResponse()

    def HealthCheck(self, request, context):
        """健康检查"""
        avg_latency = 0
        if self.total_requests > 0:
            avg_latency = self.total_latency / self.total_requests

        return classifier_pb2.HealthCheckResponse(
            healthy=True,
            model_status="loaded",
            total_requests=self.total_requests,
            avg_latency_ms=avg_latency
        )

def serve():
    """启动 gRPC 服务器"""
    # 配置日志
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # 加载配置
    model_config = ModelConfig()
    server_config = ServerConfig()

    # 创建服务器
    server = grpc.server(
        futures.ThreadPoolExecutor(max_workers=server_config.max_workers),
        options=[
            ('grpc.max_send_message_length', 50 * 1024 * 1024),
            ('grpc.max_receive_message_length', 50 * 1024 * 1024),
        ]
    )

    # 注册服务
    classifier_pb2_grpc.add_ClassifierServiceServicer_to_server(
        ClassifierServicer(model_config, server_config),
        server
    )

    # 启动服务
    server.add_insecure_port(f'{server_config.host}:{server_config.port}')
    server.start()

    logger.info(f"Classifier service started on {server_config.host}:{server_config.port}")

    try:
        server.wait_for_termination()
    except KeyboardInterrupt:
        logger.info("Shutting down server...")
        server.stop(0)

if __name__ == '__main__':
    serve()
```

**主入口** (`main.py`):
```python
#!/usr/bin/env python3
from server import serve

if __name__ == '__main__':
    serve()
```

**验收标准**:
- ✅ gRPC 服务可独立启动
- ✅ 模型加载成功
- ✅ 分类接口正常工作
- ✅ 支持健康检查

---

### Task 2.3: Go gRPC 客户端实现

**目标**: 在 Higress 插件中实现 gRPC 客户端

**客户端实现** (`client/grpc.go`):
```go
package client

import (
    "context"
    "fmt"
    "time"

    pb "github.com/alibaba/higress/plugins/golang-filter/semantic-router/proto"
    "google.golang.org/grpc"
    "google.golang.org/grpc/credentials/insecure"
)

// ClassificationResult 分类结果
type ClassificationResult struct {
    Category   string
    Confidence float64
    Scores     map[string]float64
    Method     string
    LatencyMs  int64
}

// GRPCClassifierClient gRPC 分类客户端
type GRPCClassifierClient struct {
    conn   *grpc.ClientConn
    client pb.ClassifierServiceClient
    timeout time.Duration
}

// New 创建 gRPC 客户端
func New(host string, port int, timeoutMs int, enableTLS bool) (*GRPCClassifierClient, error) {
    addr := fmt.Sprintf("%s:%d", host, port)

    // 连接选项
    opts := []grpc.DialOption{
        grpc.WithBlock(),
        grpc.WithTimeout(5 * time.Second),
    }

    if enableTLS {
        // TODO: 添加 TLS 凭证
        return nil, fmt.Errorf("TLS not implemented yet")
    } else {
        opts = append(opts, grpc.WithTransportCredentials(insecure.NewCredentials()))
    }

    // 建立连接
    conn, err := grpc.Dial(addr, opts...)
    if err != nil {
        return nil, fmt.Errorf("failed to connect to %s: %w", addr, err)
    }

    return &GRPCClassifierClient{
        conn:    conn,
        client:  pb.NewClassifierServiceClient(conn),
        timeout: time.Duration(timeoutMs) * time.Millisecond,
    }, nil
}

// Classify 分类文本
func (c *GRPCClassifierClient) Classify(ctx context.Context, text string, enableCache bool) (*ClassificationResult, error) {
    // 设置超时
    ctx, cancel := context.WithTimeout(ctx, c.timeout)
    defer cancel()

    // 调用 gRPC
    resp, err := c.client.Classify(ctx, &pb.ClassifyRequest{
        Text:        text,
        EnableCache: enableCache,
    })
    if err != nil {
        return nil, fmt.Errorf("classification failed: %w", err)
    }

    return &ClassificationResult{
        Category:   resp.Category,
        Confidence: resp.Confidence,
        Scores:     resp.Scores,
        Method:     resp.Method,
        LatencyMs:  resp.LatencyMs,
    }, nil
}

// HealthCheck 健康检查
func (c *GRPCClassifierClient) HealthCheck(ctx context.Context) error {
    ctx, cancel := context.WithTimeout(ctx, 3*time.Second)
    defer cancel()

    resp, err := c.client.HealthCheck(ctx, &pb.HealthCheckRequest{})
    if err != nil {
        return fmt.Errorf("health check failed: %w", err)
    }

    if !resp.Healthy {
        return fmt.Errorf("service unhealthy: %s", resp.ModelStatus)
    }

    return nil
}

// Close 关闭连接
func (c *GRPCClassifierClient) Close() error {
    if c.conn != nil {
        return c.conn.Close()
    }
    return nil
}
```

**验收标准**:
- ✅ gRPC 客户端可成功连接服务
- ✅ 分类调用正常
- ✅ 错误处理完善

---

### Task 2.4: 集成到 Higress 插件

**目标**: 将 gRPC 客户端集成到 filter 中

**更新 filter.go**:
```go
// DecodeData 更新为真实的 gRPC 调用
func (f *filter) DecodeData(buffer api.BufferInstance, endStream bool) api.StatusType {
    if !f.needBody {
        return api.Continue
    }

    f.bodyBuf = append(f.bodyBuf, buffer.Bytes()...)
    if !endStream {
        return api.StopAndBuffer
    }

    // 提取 prompt
    prompt := prpt.ExtractPromptFromOpenAIBody(f.bodyBuf)
    if prompt == "" {
        return api.Continue
    }

    // 缓存检查
    cacheKey := fmt.Sprintf("sr:%s", hashText(prompt))
    if v, ok := f.cache.Get(cacheKey); ok {
        if res, ok2 := v.(*client.ClassificationResult); ok2 {
            f.applyRouting(res)
            f.needBody = false
            return api.Continue
        }
    }

    // 调用 gRPC 分类服务
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()

    res, err := f.grpc.Classify(ctx, prompt, f.config.Cache.Enabled)
    if err != nil {
        api.LogWarnf("semantic-router: classify failed: %v", err)
        // 降级处理
        f.setHeaders("general", 0.5, f.config.Routing.DefaultModel, "fallback")
        f.needBody = false
        return api.Continue
    }

    // 缓存结果
    f.cache.Set(cacheKey, res)

    // 应用路由
    f.applyRouting(res)
    f.needBody = false
    return api.Continue
}

// 简单的文本哈希函数
func hashText(text string) string {
    h := fnv.New64a()
    h.Write([]byte(text))
    return fmt.Sprintf("%x", h.Sum64())
}
```

**验收标准**:
- ✅ 插件可成功调用分类服务
- ✅ 分类结果正确应用到路由
- ✅ 降级机制正常工作

---

## 测试与验证

### Test 2.1: 分类服务单元测试

**测试脚本** (`test_classifier.py`):
```python
import grpc
import classifier_pb2
import classifier_pb2_grpc

def test_classify():
    # 连接服务
    channel = grpc.insecure_channel('localhost:50051')
    stub = classifier_pb2_grpc.ClassifierServiceStub(channel)

    # 测试用例
    test_cases = [
        ("求解方程 x^2 + 2x - 3 = 0", "math"),
        ("写一个 Python 函数实现快速排序", "code"),
        ("头痛的常见原因有哪些?", "medicine"),
        ("帮我写一篇关于环保的文章", "writing"),
    ]

    for text, expected_category in test_cases:
        request = classifier_pb2.ClassifyRequest(text=text, enable_cache=False)
        response = stub.Classify(request)

        print(f"Text: {text}")
        print(f"Predicted: {response.category} (confidence: {response.confidence:.3f})")
        print(f"Expected: {expected_category}")
        print(f"Latency: {response.latency_ms}ms")
        print(f"Scores: {dict(response.scores)}")
        print("-" * 80)

        assert response.category == expected_category, f"Expected {expected_category}, got {response.category}"

if __name__ == '__main__':
    test_classify()
    print("All tests passed!")
```

### Test 2.2: 端到端集成测试

**测试流程**:
1. 启动分类服务
2. 启动 Higress (加载插件)
3. 发送测试请求
4. 验证路由头

**测试脚本** (`test/integration/test_e2e.sh`):
```bash
#!/bin/bash

# 发送测试请求
echo "Testing math question..."
response=$(curl -s -X POST http://localhost:8001/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "求解方程 x^2 + 2x - 3 = 0"}]
  }' -v 2>&1)

# 检查路由头
if echo "$response" | grep -q "X-Classification: math"; then
    echo "✓ Math classification successful"
else
    echo "✗ Math classification failed"
    exit 1
fi

if echo "$response" | grep -q "X-Target-Model: qwen-math-7b"; then
    echo "✓ Model routing successful"
else
    echo "✗ Model routing failed"
    exit 1
fi

echo "All tests passed!"
```

---

## 性能优化

### Optimization 2.1: 模型量化 (可选)

**ONNX 导出**:
```python
# export_onnx.py
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

model_path = "./models/category_classifier"
onnx_path = "./models/category_classifier/model.onnx"

# 加载模型
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)
model.eval()

# 准备示例输入
dummy_text = "This is a test sentence."
inputs = tokenizer(dummy_text, return_tensors="pt")

# 导出 ONNX
torch.onnx.export(
    model,
    (inputs['input_ids'], inputs['attention_mask']),
    onnx_path,
    input_names=['input_ids', 'attention_mask'],
    output_names=['logits'],
    dynamic_axes={
        'input_ids': {0: 'batch_size', 1: 'sequence'},
        'attention_mask': {0: 'batch_size', 1: 'sequence'},
        'logits': {0: 'batch_size'}
    },
    opset_version=14
)

print(f"Model exported to {onnx_path}")
```

### Optimization 2.2: 连接池优化

**更新 client/grpc.go**:
```go
// 使用连接池
var (
    clientPool = sync.Pool{
        New: func() interface{} {
            // 创建新连接
            return nil
        },
    }
)

// 从池中获取客户端
func (c *GRPCClassifierClient) getClient() pb.ClassifierServiceClient {
    // 实现连接池逻辑
    return c.client
}
```

---

## 里程碑验收

### 验收清单

- [ ] **分类服务**
  - [ ] gRPC 服务可独立运行
  - [ ] 模型推理正常
  - [ ] 性能满足要求 (< 30ms p99)
  - [ ] 健康检查正常

- [ ] **客户端集成**
  - [ ] gRPC 客户端实现完整
  - [ ] 插件可正常调用服务
  - [ ] 错误处理完善

- [ ] **测试**
  - [ ] 单元测试通过
  - [ ] 集成测试通过
  - [ ] 性能测试达标

### 交付物

1. **分类服务**: `classifier-service/`
2. **gRPC 客户端**: `client/grpc.go`
3. **Protocol Buffer**: `proto/classifier.proto`
4. **测试脚本**: `test/`
5. **文档**: `README_CLASSIFIER.md`

---

## 下一步

完成阶段二后,进入 [阶段三: 模型训练](./04-阶段三-模型训练.md)
